{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "CzBaUOAbhfX1",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Week 13 - Machine learning computer exercises\n",
    "\n",
    "In this exercise set you will practice using simple supervised machine learning models (**Part 1**) and ensemble machine learning models (**Part 2**). You will be using these models to perform some classification tasks and evaluating their performance. Proceed through the notebook below and report your results in **Week 13. Assignment 1** in Moodle where indicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "VHInU9YFhfX4",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Background\n",
    "\n",
    "### What data will we be using and where is it from?\n",
    "We will be utilizing **gene expression data** from the breast cancer study of [The Cancer Genome Atlas (TCGA)](https://cancergenome.nih.gov/). You can find the publication associated with this study [here](https://www.nature.com/articles/nature11412). TCGA is a landmark cancer genomics program that has molecularly characterized over 20,000 cancer samples from 33 cancer types. The aim of their breast cancer study was to perform comprehensive molecular profiling of human breast tumors by integrating multiple data types, including DNA copy number, DNA methylation, exome sequencing, protein assays, and gene expression information. In this exercise set we will only be focusing on the gene expression data.\n",
    "\n",
    "### What is breast cancer?\n",
    "Breast cancer is the most common form of invasive cancer among women (~7.6M cases worldwide annually, 450K deaths worldwide annually). Breast cancer is a heterogenous disease, meaning that there exist distinct molecular subtypes of breast cancer, some of which are also good predictors of response to treatment. In current routine diagnostics, immunohistochemistry (IHC) is used to determine the presence of routine markers that are used clinically, including:\n",
    "\n",
    "* Estrogen receptor status (ER)\n",
    "* Progesterone receptor status (PR)\n",
    "* Human epidermal growth factor receptor 2 status (HER2)\n",
    "\n",
    "### Our objective\n",
    "\n",
    "The breast cancer dataset from TCGA consists of gene expression values for many breast cancer samples. The gene expression data has already been processed and normalized and is ready for us to use for our machine learning tasks. The dataset also contains the status of each sample for ER, PR, and HER2 as described above (assessed by detecting protein expression using IHC). **In this exercise, our objective is to use machine learning approaches to predict the PR status of each breast cancer sample based on its gene expression profile.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXUu4vMphfX5"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by loading our input data. The data is provided in an Rdata file, which is a file type specific to the R programming language used to store multiple data objects (e.g. matrices and vectors) within a single file. Luckily, we can also load Rdata files using Python using the `pyreadr` module.\n",
    "\n",
    "Run the cell below to load the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "9EVYkDTYhfX6",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%pip install pyreadr # Install the pyreadr module\n",
    "import pyreadr # Import the module for use\n",
    "input_data = pyreadr.read_r(\"tcga_breast_prediction_train_and_test.Rdata\")\n",
    "\n",
    "# Separate out the objects within the Rdata file\n",
    "train_data = input_data[\"x\"]\n",
    "train_meta = input_data[\"meta\"]\n",
    "test_data = input_data[\"xTest\"]\n",
    "test_meta = input_data[\"metaTest\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypjIjBSThfX7"
   },
   "source": [
    "The input data we have just loaded contains 4 objects:\n",
    "* `train_data` --> this is the **training data matrix** (breast cancer samples as rows, genes for which we have expression values as columns)\n",
    "* `train_meta` --> metadata for the training dataset (which we will extract training labels from)\n",
    "* `test_data` --> this is the **test data matrix** (samples as rows, genes as columns)\n",
    "* `test_meta` --> metadata for the test dataset (which we will extract test labels from)\n",
    "\n",
    "**Question 1:** We have selected a subset of samples and genes for the train and test sets to cut down on computation time. Using commands you learned during the exercises of **Week 1** and **Week 2** (hint: you can use the `pandas` module here), determine how many breast cancer samples we have data for and how many genes have available gene expression measurements in our **training dataset**. Report your answers in Moodle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8kb4_JFShfX9"
   },
   "outputs": [],
   "source": [
    "# Enter your commands here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Pa2QC2sBhfX9",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## PART 1: NEAREST-NEIGHBOR MACHINE LEARNING MODELS\n",
    "\n",
    "The data matrix `train_data` contains gene expression measurements that you will use as predictors. The data matrix `train_meta` contains the response variable that we will predict, which will be the **PR (progesterone receptor) status of each breast cancer sample**.\n",
    "\n",
    "Using the code cell below, determine which columns the data matrices `train_meta` and `test_meta` contain. After this, select the column named `pr` in the `train_meta` and `test_meta` matrices and assign them to new variables `status` and `status_test`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "pgyahAvuhfX-",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Use the pandas module\n",
    "import pandas as pd\n",
    "\n",
    "# Start by checking the column names in train_meta and test_meta\n",
    "list(train_meta)\n",
    "list(test_meta)\n",
    "\n",
    "# Edit the code below to assign the column pr to variable status and status_test\n",
    "status = train_meta[EDIT_HERE] # Training data\n",
    "status_test = test_meta[EDIT_HERE] # Testing data\n",
    "\n",
    "# What types of PR status information is available?\n",
    "print(status.unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "53BrT7gBhfX_",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Exclude training and test data labeled as `Indeterminate` for PR status\n",
    "\n",
    "For our analyses here, we are only interested in positive and negative PR status. As you saw above, our `status` variable contains Negative, Positive, and Indeterminate (i.e., cannot be determined) labels for PR status. Let's exclude samples labeled as Indeterminate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "6gxsZOxXhfYA",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Determine the index in status that are labeled as Indeterminate:\n",
    "ind = status != 'Indeterminate'\n",
    "print(f\"{ind.sum()} training set samples are NOT labeled as PR status Indeterminate\")\n",
    "\n",
    "# Then select only observations that are not Indeterminate in train_data, train_meta, and status:\n",
    "status = status[ind].reset_index(drop=True)\n",
    "train_data = train_data[ind.values].reset_index(drop=True)\n",
    "train_meta = train_meta[ind.values].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-Twne1EhfYA"
   },
   "source": [
    "**Question 2:** How many breast cancer training samples had the label Indeterminate for their PR status? Report your answer in Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0AQ2rEA6Ls5"
   },
   "source": [
    "Remove the Indeterminate samples also from the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AFCniRac6JhB"
   },
   "outputs": [],
   "source": [
    "# Determine the index in status that are labeled as Indeterminate:\n",
    "ind = status_test != 'Indeterminate'\n",
    "print(f\"{ind.sum()} test set samples are NOT labeled as PR status Indeterminate\")\n",
    "\n",
    "# Then select only observations that are not Indeterminate in test_data, test_meta, and status_test:\n",
    "status_test = status_test[ind].reset_index(drop=True)\n",
    "test_data = test_data[ind.values].reset_index(drop=True)\n",
    "test_meta = test_meta[ind.values].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "xaoPRY3DhfYB",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Fitting a k-nearest neighbor (KNN) classification model\n",
    "\n",
    "[K-nearest neighbor (KNN)](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) is a non-parametric supervised machine learning algorithm that we can use to perform **classification** tasks. This means that the algorithm tries to predict the correct label (for us, PR status) of a given input data (for us, a breast cancer sample with a particular gene expression profile). KNN does this based on proximity, i.e. by classifying each input sample by a plurality vote of its neighbors, with the sample being assigned to the label most common among its *k* nearest neighbors in the KNN model's training data (where *k* is a positive integer).\n",
    "\n",
    "In the code cell below, let's train a KNN model using the training data. To illustrate how the model works, we can then use the model to predict the PR status of our training data samples.\n",
    "\n",
    "You will notice that we use a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) to look at the model's performance in terms of true negatives, false positives, false negatives, and true positives. We also calculate the model's [accuracy](https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall), which is the proportion of all classifications that were correct, whether positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "TPZj_143hfYC",
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install scikit-learn # Install the scikit-learn module that contains the KNN model\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix # For evaluating model performance\n",
    "np.random.seed(12345) # Set a seed for reproducible results\n",
    "\n",
    "k = 5 # This is the parameter k in the KNN model\n",
    "\n",
    "# Define the KNN model with k neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn.fit(train_data, status)\n",
    "\n",
    "# Predict the PR status of training data samples\n",
    "predicted_status = knn.predict(train_data)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(status, predicted_status)\n",
    "\n",
    "# Extract the true negatives, false positives, false negatives, and true positives\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Calculate accuracy, sensitivity, and specificity\n",
    "accuracy = sum(predicted_status == status) / len(status)\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(f'k={k} accuracy={accuracy} sensitivity={sensitivity} specificity={specificity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOJq37AZhfYC"
   },
   "source": [
    "**Question 3:** Using *k=5*, how many breast cancer patients are **predicted** by our model to be PR positive and negative? Report your answers in Moodle.\n",
    "\n",
    "**Hint:** In the cell below you can find commands for determining the number of truly PR positive and negative patients in the training set. Modify these commands to determine the number of PR positive and negative patients in the **model predictions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0k3a2QQ4hfYC"
   },
   "outputs": [],
   "source": [
    "print((status == \"Positive\").sum())\n",
    "print((status == \"Negative\").sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9UjwlQf6hfYD"
   },
   "source": [
    "**Questions 4 and 5:** What happens to the model performance if you change the *k* parameter to be 1? Why does this happen? What are the accuracy, sensitivity, and specificity? Report you answers in Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "1BMm48A8hfYE",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Using cross-validation for optimizing *k* in the KNN classifier\n",
    "\n",
    "In the section above, we evaluated the accuracy, sensitivity, and specificity of the KNN classifier at two different values of *k* when predicting the PR status of samples in the **training dataset**. We would like to choose a value for *k* that results in the best model performance.\n",
    "\n",
    "However, as you learned during the lecture, supervised prediction models can easily be overfitted, and care has to be taken to avoid this. One way to reduce the risk of overfitting when optimizing the complexity of the model (here denoted by *k*), is to use cross-validation.\n",
    "\n",
    "In the code cell below, you are provided with the code for separating data into training and test cross-validation sets, together with a `for-loop` for the cross-validation. The cross-validation implemented below is [leave-one-out cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "iH20CPSWhfYE",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "np.random.seed(12345) # Set a seed for reproducible results\n",
    "\n",
    "k = 5 # Set a value for the number of neighbors, k\n",
    "\n",
    "n = len(train_data) # Number of samples in the train dataset\n",
    "\n",
    "true_pr_status_save = [] # Empty vector for storing true PR status labels\n",
    "predicted_pr_status_save = [] # Empty vector for storing predicted PR status labels\n",
    "\n",
    "# This loop is for the cross-validation process\n",
    "for i in range(n): # Loop through all samples in the training dataset\n",
    "    # Define samples to be used for training and testing\n",
    "    index_test = [i]  # This is the observation index that will be used as test\n",
    "    index_training = list(set(range(n)) - set(index_test))  # These are the observations used as the training set\n",
    "\n",
    "    # Generate training and testing data matrices and PR status labels\n",
    "    x_training = train_data.iloc[index_training, :]  # Training set gene expression values\n",
    "    y_training = status.iloc[index_training]  # Training set PR status labels\n",
    "    x_test = train_data.iloc[index_test, :]  # Test set gene expression values\n",
    "    y_test = status.iloc[index_test]  # Test set PR status labels\n",
    "\n",
    "    # Model training and testing\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_training, y_training)\n",
    "    predicted_label = knn.predict(x_test)\n",
    "    predicted_pr_status_save.extend(predicted_label)  # Save predicted PR status\n",
    "    true_pr_status_save.extend(y_test)  # Save true PR status\n",
    "\n",
    "# Convert lists to arrays for metrics calculation\n",
    "predicted_pr_status_save = np.array(predicted_pr_status_save)\n",
    "true_pr_status_save = np.array(true_pr_status_save)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(true_pr_status_save, predicted_pr_status_save)\n",
    "\n",
    "# Extract the true negatives, false positives, false negatives, and true positives\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Calculate sensitivity and specificity\n",
    "accuracy = sum(predicted_pr_status_save == true_pr_status_save) / len(true_pr_status_save)\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(f'accuracy = {accuracy}')\n",
    "print(f'sensitivity = {sensitivity}')\n",
    "print(f'specificity = {specificity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_npIBlKhfYF"
   },
   "source": [
    "**Question 6:** How does the performance of the model (accuracy, sensitivity, and specificity) using cross-validation and *k=5* compare to the performance of the model without cross-validation and *k=5* from the previous section? Report your answer in Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kgAm1ciQhfYF"
   },
   "source": [
    "Let's modify the cross-validation code above so that we can evaluate the accuracy of the model at different values of *k*. We can use the code below to perform a [grid search](https://en.wikipedia.org/wiki/Hyperparameter_optimization) between *k=1* to *k=100* to determine which value of *k* produces the best model performance. **Please note that this code can take a few minutes to run!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "sabVJqwLhfYF",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "np.random.seed(12345) # Set a seed for reproducible results\n",
    "\n",
    "# Evaluate cross-validation results for k = 1 to k = 100 with increments of 3.\n",
    "kmin = 1 # Minimim k to evaluate\n",
    "kmax = 100 # Maximum k to evaluate\n",
    "result_matrix = np.zeros((0,2)) # Initialize empty matrix with zero rows and two columns\n",
    "for k in range(kmin, kmax + 1, 3):\n",
    "    n = len(train_data) # Number of samples in the train dataset\n",
    "    true_pr_status_save = [] # Empty vector for storing true PR status labels\n",
    "    predicted_pr_status_save = [] # Empty vector for storing predicted PR status labels\n",
    "\n",
    "    for i in range(n): # Loop through all samples in the training dataset\n",
    "        # Define samples to be used for training and testing\n",
    "        index_test = [i]  # This is the observation index that will be used as test\n",
    "        index_training = list(set(range(n)) - set(index_test))  # These are the observations used as the training set\n",
    "\n",
    "        # Generate training and testing data matrices and PR status labels\n",
    "        x_training = train_data.iloc[index_training, :]  # Training set gene expression values\n",
    "        y_training = status.iloc[index_training]  # Training set PR status labels\n",
    "        x_test = train_data.iloc[index_test, :]  # Test set gene expression values\n",
    "        y_test = status.iloc[index_test]  # Test set PR status labels\n",
    "\n",
    "        # Model training and testing\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(x_training, y_training)\n",
    "        predicted_label = knn.predict(x_test)\n",
    "        predicted_pr_status_save.extend(predicted_label)  # Save predicted PR status\n",
    "        true_pr_status_save.extend(y_test)  # Save true PR status\n",
    "\n",
    "    # Convert lists to arrays for metrics calculation\n",
    "    predicted_pr_status_save = np.array(predicted_pr_status_save)\n",
    "    true_pr_status_save = np.array(true_pr_status_save)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = sum(predicted_pr_status_save == true_pr_status_save) / len(true_pr_status_save)\n",
    "\n",
    "    prediction_results = [k, accuracy]\n",
    "    result_matrix = np.vstack([result_matrix, prediction_results])\n",
    "\n",
    "print(result_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "snVOVccHhfYG",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Then we can plot the results from the grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "_IM6n2dwhfYG",
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract accuracy from the result_matrix\n",
    "accuracy = result_matrix[:,1]\n",
    "k_values = result_matrix[:,0]\n",
    "\n",
    "# Plot the accuracy\n",
    "plt.plot(k_values, accuracy, color='black', label='Accuracy')  # Plot accuracy in black\n",
    "\n",
    "# Set plot limits and labels\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('accuracy')\n",
    "\n",
    "# Add grid and legend\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "xBwicpJ0hfYG",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**Question 7:** Looking at model accuracy, let's choose the value of *k* that maximizes accuracy of the KNN model with our data. The command below helps you find the index of the *k* value corresponding to the maximum accuracy in the variable *result_matrix*. What is the value of *k* corresponding to this index of the matrix? Remember what you learned about slicing (also called indexing) in Python during the **Week 2** exercises. Report your answer in Moodle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYRKbL2SzemN"
   },
   "outputs": [],
   "source": [
    "# argmax finds the index of the row in result_matrix where maximum accuracy was observed.\n",
    "# The accuracies are stored in column 1 of the matrix.\n",
    "best_ind = np.argmax(result_matrix[:,1])\n",
    "# Find the corresponding value of k stored in column 0 of the matrix on the row\n",
    "# indicated by best_ind.\n",
    "best_k = # EDIT HERE TO OBTAIN THE K VALUE BY SLICING/INDEXING THE MATRIX\n",
    "print(\"Best value for k was: \"+str(int(best_k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ltHQLZNs89zh",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Final evaluation of the optimized KNN classifier on test data\n",
    "\n",
    "Now that we have used the cross-validation approach on training data to find an optimal design for our KNN classifier, we can proceed to train our final model on the full training dataset. In the case of KNN, this is mainly a matter of selecting an optimal value for *k*, but more complex models may have hundreds of tunable settings and parameters. After this, the test data can be accessed to evaluate how the final model will perform on new, unseen data to give us a realistic estimate of how the model will work in \"the real world\".\n",
    "\n",
    "Ideally, this process should only be conducted once to avoid overtweaking the model design to perform optimally on the test data. **It is considered bad practice in machine learning to go back to changing the model design (in our case the parameter *k*) after we have accessed the held-out test data and seen how the model performs on unseen data.** There is a risk that tweaking the model to perform optimally on this test dataset will not translate to good performance on other future test sets (i.e., the model does not *generalize*). If we utilize information from this test dataset to further tweak the model design, we should collect another new unseen test dataset to evaluate the performance of the updated model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yx27FHbBBs1"
   },
   "source": [
    "Here is the code we used earlier to train and evaluate the KNN model on the training data. Edit the code (see the lines indicated with \"EDIT HERE\") to train the model with the optimal value of *k* on the training dataset, and to run the predictions and evaluation on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXpkMkbA9sOy"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix # For evaluating model performance\n",
    "np.random.seed(12345) # Set a seed for reproducible results\n",
    "\n",
    "k = # EDIT HERE TO USE THE OPTIMAL VALUE OF K\n",
    "\n",
    "# Define the KNN model with k neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn.fit(train_data, status)\n",
    "\n",
    "# Predict the PR status of test data samples\n",
    "predicted_status = # EDIT HERE TO RUN PREDICTIONS ON TEST DATA\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = # EDIT HERE TO CALCULATE CONFUSION MATRIX USING THE PREDICTED AND TRUE STATUS FOR TEST DATA\n",
    "\n",
    "# Extract the true negatives, false positives, false negatives, and true positives\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Calculate accuracy, sensitivity, and specificity\n",
    "accuracy = # EDIT HERE TO CALCULATE ACCURACY FOR TEST DATA\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(f'k={k} accuracy={accuracy} sensitivity={sensitivity} specificity={specificity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "aVUcRPvoChcF",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**Question 8:** How did the final model perform on the test dataset? Report the accuracy with three decimals in Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qI4qeOJ1JbCF"
   },
   "source": [
    "## PART 2: ENSEMBLE MACHINE LEARNING MODELS\n",
    "\n",
    "In this second part of the exercises, we will use **ensemble machine learning models** to predict the PR status of breast cancer samples based on their gene expression profiles. [Ensemble learning](https://en.wikipedia.org/wiki/Ensemble_learning) trains two or more machine learning algorithms on a specific classification task, with each algorithm within the ensemble model referred to as a *base learner*. Ensemble learning is based on the idea that while a single base learner may have poor predictive ability, multiple base learners together will perform better.\n",
    "\n",
    "The model we will focus on is the **gradient boosted tree**, which uses an ensemble of **decision trees** as base learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zg0-VOXNmahh"
   },
   "source": [
    "### Training an XGBoost model and running predictions on test data\n",
    "Using the same TCGA breast cancer gene expression data (make sure you have run the code cells up to where we exclude the samples labeled as Indeterminate for PR status from the training and testing data!), let's predict PR status using gradient boosted trees implemented with the popular XGBoost library. XGBoost is a good starting point for any modern machine learning project.\n",
    "\n",
    "Let's first train an XGBoost model using the same training dataset as for the KNN model and then run predictions on the test set before we analyze them further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6BhJHZKttRZz"
   },
   "outputs": [],
   "source": [
    "!pip install xgboost\n",
    "import xgboost as xgb\n",
    "import random\n",
    "random.seed(12345) # Set a seed for reproducible results\n",
    "\n",
    "# Convert status_to numerical labels, where 1 indicates positive for PR and\n",
    "# 0 indicates negative for PR.\n",
    "numerical_status = [1 if s == \"Positive\" else 0 for s in status]\n",
    "numerical_status_test = [1 if s == \"Positive\" else 0 for s in status_test]\n",
    "\n",
    "# Convert data to XGBoost's DMatrix format\n",
    "dtrain = xgb.DMatrix(train_data, label=numerical_status)\n",
    "\n",
    "# Define the most important XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'binary:logistic',  # For binary classification\n",
    "    'eval_metric': 'logloss',  # Evaluation metric\n",
    "    'eta': 0.1,  # Learning rate\n",
    "    'max_depth': 3,  # Maximum tree depth\n",
    "    'subsample': 0.8, # Subsample ratio of the training instances\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Train the XGBoost model for 50 iterations\n",
    "model = xgb.train(params, dtrain, num_boost_round=50)\n",
    "\n",
    "# Convert test data to DMatrix format\n",
    "dtest = xgb.DMatrix(test_data)\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions = model.predict(dtest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFy_5MCtW4PZ"
   },
   "source": [
    "### Performing ROC analysis for a probabilistic model\n",
    "Like many machine learning models (with KNN being an exception), the predictions XGBoost outputs are probabilities. The variable *predictions* now contains the predicted probabilities for each test sample. A value of 0 means it's very unlikely that the sample is positive for PR and a value of 1 means it's very likely that the sample is positive.\n",
    "\n",
    "But how certain do we need to be to consider a sample to be positive? To classify each sample as either positive or negative for PR status, we can use different cutoff thresholds for the probabilities. A typical default is 0.50, meaning samples with >= 50% probability of being positive are classified as positive, while any samples with <50% probability are classified as negative.\n",
    "\n",
    "In different applications, we may however want to select another threshold to minimize false negatives or false positives. For example, we might want to use a model in medical diagnostics in a way that prioritises sensitivity to minimize false negatives (due to the potentially serious consequences of failing to detect a serious disease). This would be achieved by using a lower threshold for calling a sample positive, e.g. 0.20. The tradeoff is that specificity will then be decreased, leading to more \"false alarms\". This may however be acceptable, if the clinician can then use other confirmatory tests to avoid misdiagnosing a healthy patient with the disease.\n",
    "\n",
    "A useful tool for analyzing the performance of a prediction model across all possible thresholds is the Receiver Operating Characteristic (ROC) curve. Each point on a ROC curve shows the sensitivity and specificity of the model with a given threshold value, and the curve is obtained by varying the threshold from 0 to 1. Let's plot the ROC curve and tabulate the underlying sensitivity & specificity & threshold combinations for our XGBoost PR status model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KDU2mSw1aVeP"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Convert lists with the predicted and true PR status to arrays for calculations\n",
    "predictions = np.array(predictions)\n",
    "numerical_status_test = np.array(numerical_status_test)\n",
    "\n",
    "# Calculate the points of the ROC curve. For each of the evaluated probability\n",
    "# thresholds, we calculate the False Positive Rate (FPR) which equals 1 - specificity\n",
    "# and the True Positive Rate, which is just another name for sensitivity.\n",
    "fpr, tpr, thresholds = roc_curve(numerical_status_test, predictions)\n",
    "\n",
    "# We can additionally calculate Area Under the Curve (AUC) to summarize the\n",
    "# overall performance of the model across all thresholds.\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('1 - Specificity')\n",
    "plt.ylabel('Sensitivity')\n",
    "plt.title('ROC Curve for XGBoost PR Status Prediction')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4TjR8kdLkqFe"
   },
   "source": [
    "**Question 9:** Each point on the ROC curve represents a specific threshold and a resulting tradeoff between sensitivity and specificity. Note that it is customary to use an inverted x-axis by plotting 1-specificity (i.e. False Positive Rate) instead of specificity. How would the ROC curve look like for a very well performing model? Report your answer in Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PSNf8kRmAqR"
   },
   "source": [
    "### Adjusting decision thresholds for a probabilistic model\n",
    "Based on he ROC analysis, we can pick a suitable probability threshold for classifying each predicted sample as positive or negative for PR status. We can analyze the ROC curve for a compromise between sensitivity and specificity that we consider optimal for our application. In a real-world scenario, this could involve discussions about the consequences of false positives vs false negatives in terms of costs or even ethical considerations.\n",
    "\n",
    "Let's assume we need to obtain a sensitivity of at least 95% in our task of predicting which samples are positive for PR status. That is, we accept missing 5% of the positive samples. Using the code below, we can examine the data underlying the ROC curve in tabular form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECsccVxjkxAw"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from the ROC curve data\n",
    "roc_df = pd.DataFrame({'Threshold': thresholds, 'Specificity': 1-fpr, 'Sensitivity': tpr})\n",
    "\n",
    "# Display the DataFrame as a table\n",
    "roc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEMOtsPOn3Z8"
   },
   "source": [
    "**Question 10:** Which threshold would you pick to achieve at least 95% sensitivity and what will be the resulting specificity of the model? Report your answer in Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8t1M7VEoPvs"
   },
   "source": [
    "**Question 11:** Finally, apply the threshold you selected to the probabilities predicted by the model to obtain a final classification (1 meaning PR-positive vs. 0 meaning PR-negative) for each test sample. Edit the code below to calculate how many samples in the test set were predicted to be positive at this threshold. Report your answer in Moodle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLMmXa3waM9o"
   },
   "outputs": [],
   "source": [
    "# Convert probabilities to class labels (0 or 1)\n",
    "threshold = # EDIT HERE TO SET YOUR PROBABILITY THRESHOLD\n",
    "predicted_labels = [1 if p > threshold else 0 for p in predictions]\n",
    "\n",
    "n_positives = # EDIT HERE TO CALCULATE HOW MANY SAMPLES WERE PREDICTED POSITIVE\n",
    "\n",
    "print(\"The model predicted \"+str(n_positives)+\" positive samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0352IOwqFNB"
   },
   "source": [
    "### Analyzing the importance of predictor variables\n",
    "Some models, including XGBoost, allow estimating which of the input variables had the highest importance for the prediction. In statistical circles the input variables of a predictive model would usually be called *predictors*, while in the machine learning community they are often called *features*. By analyzing which features/predictors contain the most useful information for the prediction task of interest, we can both troubleshoot poorly performing models and sometimes even make new discoveries.\n",
    "\n",
    "Let's analyze which of the genes in our dataset are most informative for the trained XGBoost model in predicting PR status. There are different ways of defining importance and we will not go into the details, but here we will use the **\"gain\" based importance score**.\n",
    "\n",
    "Using the code below, calculate the relative importance of the features and examine the top-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WVvS92jFqEAS"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get feature importance scores\n",
    "importance_scores = model.get_score(importance_type='gain')\n",
    "\n",
    "# Convert to DataFrame for easy sorting\n",
    "importance_df = pd.DataFrame({'Feature': importance_scores.keys(), 'Importance': importance_scores.values()})\n",
    "\n",
    "# Sort by importance\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Get top 10\n",
    "top_10_features = importance_df.head(10)\n",
    "\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_10_features['Feature'], top_10_features['Importance'])\n",
    "plt.xlabel('Feature (Gene)')\n",
    "plt.ylabel('Importance Score')\n",
    "plt.title('Top 10 Most Important Features for PR Status Prediction')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for readability\n",
    "plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKsIt_9l1K_O"
   },
   "source": [
    "**Question 12:** The labels shown on the plot above are Ensembl gene identifiers. Search for at least the top-3 most predictive genes in [the NCBI Gene database](https://www.ncbi.nlm.nih.gov/gene/). Report the official gene symbols for the three genes in Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4t3q85wT4wme"
   },
   "source": [
    "**Question 13:** What can you conclude based on the identities of the three genes whose gene expression is most predictive of progesterone receptor status in these data? Remember that the PR status in this dataset was established by detecting the presence or absence of PR proteins using immunohistochemistry. Report your answer in Moodle."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
